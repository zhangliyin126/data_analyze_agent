from __future__ import annotations

from typing import Any, Dict, List


def _safe_error(exc: Exception, limit: int = 240) -> str:
    text = f"{type(exc).__name__}: {exc}"
    if len(text) <= limit:
        return text
    return text[: limit - 3] + "..."


class LLMOrchestrator:
    """Central place for planner/interpreter/analyst LLM calls."""

    def __init__(self, llm_client: Any, observer: Any | None = None) -> None:
        self.llm_client = llm_client
        self.observer = observer

    def _emit(self, run_id: str, event: Dict[str, Any]) -> None:
        if self.observer is None or not run_id:
            return
        self.observer.emit_stream_event(run_id, event)

    @staticmethod
    def _fallback_plan(candidates: Dict[str, Any], profile: Dict[str, Any]) -> Dict[str, Any]:
        task_candidates = candidates.get("task_candidates", [])
        task = task_candidates[0]["task_type"] if task_candidates else "eda"
        target_candidates = candidates.get("target_candidates", [])
        structure_info = candidates.get("data_structure", {})
        structure = structure_info.get("data_structure", "flat_iid")
        signals = structure_info.get("signals", {})
        schema_groups = signals.get("schema_group_cols", [])
        profile_groups = profile.get("group_candidates", [])
        group_column = schema_groups[0] if schema_groups else (profile_groups[0] if profile_groups else None)
        time_columns = profile.get("time_columns", [])
        time_column = time_columns[0] if time_columns else None
        return {
            "task_type": task,
            "data_structure": structure,
            "target_column": target_candidates[0] if target_candidates else None,
            "only_eda": task == "eda",
            "use_mixed_effect": bool(candidates.get("must_use_mixed_effect", False)),
            "group_column": group_column,
            "time_column": time_column,
            "feature_strategy": {"encoding": "auto", "scaling": "standard"},
            "modeling_strategy": {"baseline_models": ["default"]},
            "evaluation_strategy": {"metrics": "auto"},
            "reasoning": ["llm planner failed, fallback plan generated by orchestration layer"],
        }

    def plan(
        self,
        run_id: str,
        profile: Dict[str, Any],
        schema: Dict[str, Any],
        candidates: Dict[str, Any],
        user_intent: str,
    ) -> Dict[str, Any]:
        self._emit(
            run_id,
            {
                "event": "llm_call_start",
                "node": "plan",
                "role": "planner",
            },
        )
        try:
            plan = self.llm_client.plan(profile, schema, candidates, user_intent)
            if not isinstance(plan, dict):
                raise TypeError("planner output must be a dict")
            self._emit(
                run_id,
                {
                    "event": "llm_call_success",
                    "node": "plan",
                    "role": "planner",
                    "keys": sorted(plan.keys()),
                },
            )
            return plan
        except Exception as exc:  # pragma: no cover - depends on llm runtime stability
            fallback = self._fallback_plan(candidates, profile)
            self._emit(
                run_id,
                {
                    "event": "llm_call_failed",
                    "node": "plan",
                    "role": "planner",
                    "error": _safe_error(exc),
                    "fallback": True,
                },
            )
            return fallback

    def interpret_stage(self, run_id: str, stage: str, stage_result: Dict[str, Any]) -> Dict[str, Any]:
        self._emit(
            run_id,
            {
                "event": "llm_call_start",
                "node": stage,
                "role": "interpreter",
            },
        )
        try:
            payload = self.llm_client.interpret_stage(stage, stage_result)
            if not isinstance(payload, dict):
                raise TypeError("interpreter output must be a dict")
            key_findings = payload.get("key_findings", [f"{stage} completed"])
            risks = payload.get("risks", [])
            next_step = payload.get("next_step", "continue")
            normalized = {
                "key_findings": key_findings if isinstance(key_findings, list) else [str(key_findings)],
                "risks": risks if isinstance(risks, list) else [str(risks)],
                "next_step": str(next_step),
            }
            self._emit(
                run_id,
                {
                    "event": "llm_call_success",
                    "node": stage,
                    "role": "interpreter",
                },
            )
            return normalized
        except Exception as exc:  # pragma: no cover - depends on llm runtime stability
            self._emit(
                run_id,
                {
                    "event": "llm_call_failed",
                    "node": stage,
                    "role": "interpreter",
                    "error": _safe_error(exc),
                    "fallback": True,
                },
            )
            return {
                "key_findings": [f"{stage} completed"],
                "risks": ["interpreter unavailable, fallback summary used"],
                "next_step": "continue",
            }

    def answer(
        self,
        run_id: str,
        question: str,
        artifact_summaries: List[Dict[str, Any]],
        final_summary: str,
    ) -> str:
        self._emit(
            run_id,
            {
                "event": "llm_call_start",
                "node": "qa",
                "role": "analyst",
            },
        )
        try:
            answer = self.llm_client.answer(question, artifact_summaries, final_summary)
            if not isinstance(answer, str):
                answer = str(answer)
            self._emit(
                run_id,
                {
                    "event": "llm_call_success",
                    "node": "qa",
                    "role": "analyst",
                },
            )
            return answer
        except Exception as exc:  # pragma: no cover - depends on llm runtime stability
            self._emit(
                run_id,
                {
                    "event": "llm_call_failed",
                    "node": "qa",
                    "role": "analyst",
                    "error": _safe_error(exc),
                    "fallback": True,
                },
            )
            return f"LLM QA unavailable. Fallback to final summary.\n{final_summary}"
